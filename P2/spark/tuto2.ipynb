{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=test, master=local[*]) created by __init__ at <ipython-input-1-6e82a18047ae>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e82a18047ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msqlCtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=test, master=local[*]) created by __init__ at <ipython-input-1-6e82a18047ae>:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\")\n",
    "sc = SparkContext(conf=conf)\n",
    "                              \n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.load('file:///home/fran/Descargas/p2/Splitted/task3_dataset_cortado.csv', \n",
    "                     format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "df2 = sqlCtx.read.load('file:///home/fran/Descargas/p2/Splitted/task3_dataset_cortado_solo_ataques.csv', \n",
    "                     format='com.databricks.spark.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'GyroscopeStat_x_MEAN',\n",
       " 'GyroscopeStat_z_MEAN',\n",
       " 'GyroscopeStat_COV_z_x',\n",
       " 'GyroscopeStat_COV_z_y',\n",
       " 'MagneticField_x_MEAN',\n",
       " 'MagneticField_z_MEAN',\n",
       " 'MagneticField_COV_z_x',\n",
       " 'MagneticField_COV_z_y',\n",
       " 'Pressure_MEAN',\n",
       " 'LinearAcceleration_COV_z_x',\n",
       " 'LinearAcceleration_COV_z_y',\n",
       " 'LinearAcceleration_x_MEAN',\n",
       " 'LinearAcceleration_z_MEAN',\n",
       " 'attack']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- GyroscopeStat_x_MEAN: double (nullable = true)\n",
      " |-- GyroscopeStat_z_MEAN: double (nullable = true)\n",
      " |-- GyroscopeStat_COV_z_x: double (nullable = true)\n",
      " |-- GyroscopeStat_COV_z_y: double (nullable = true)\n",
      " |-- MagneticField_x_MEAN: double (nullable = true)\n",
      " |-- MagneticField_z_MEAN: double (nullable = true)\n",
      " |-- MagneticField_COV_z_x: double (nullable = true)\n",
      " |-- MagneticField_COV_z_y: double (nullable = true)\n",
      " |-- Pressure_MEAN: double (nullable = true)\n",
      " |-- LinearAcceleration_COV_z_x: double (nullable = true)\n",
      " |-- LinearAcceleration_COV_z_y: double (nullable = true)\n",
      " |-- LinearAcceleration_x_MEAN: double (nullable = true)\n",
      " |-- LinearAcceleration_z_MEAN: double (nullable = true)\n",
      " |-- attack: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_c0</th>\n",
       "      <td>38</td>\n",
       "      <td>24498.5</td>\n",
       "      <td>11.113055385446435</td>\n",
       "      <td>24480</td>\n",
       "      <td>24517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyroscopeStat_x_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.002919810384414524</td>\n",
       "      <td>0.042653074223189466</td>\n",
       "      <td>-0.12934446667563454</td>\n",
       "      <td>0.192444842939196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyroscopeStat_z_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.020669563899161792</td>\n",
       "      <td>0.06558910434958976</td>\n",
       "      <td>-0.4005862796940704</td>\n",
       "      <td>0.01580415402964103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyroscopeStat_COV_z_x</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.004745679981555173</td>\n",
       "      <td>0.03900978043943563</td>\n",
       "      <td>-0.2293913810431748</td>\n",
       "      <td>0.05479838414699555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyroscopeStat_COV_z_y</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0027225391041282225</td>\n",
       "      <td>0.04253071796921174</td>\n",
       "      <td>-0.09230434233181373</td>\n",
       "      <td>0.23981542270362485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MagneticField_x_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-828.5815522391483</td>\n",
       "      <td>5068.768853339744</td>\n",
       "      <td>-31252.0</td>\n",
       "      <td>42.310158730158726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MagneticField_z_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-20.569553774133187</td>\n",
       "      <td>16.276224724990833</td>\n",
       "      <td>-61.70115789473685</td>\n",
       "      <td>7.576736842105262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MagneticField_COV_z_x</th>\n",
       "      <td>38</td>\n",
       "      <td>3.6145847662909802</td>\n",
       "      <td>10.42308323200004</td>\n",
       "      <td>-45.43701104356637</td>\n",
       "      <td>29.967837894736828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MagneticField_COV_z_y</th>\n",
       "      <td>38</td>\n",
       "      <td>4.417610595486042</td>\n",
       "      <td>6.088725194969065</td>\n",
       "      <td>-7.984286527514232</td>\n",
       "      <td>22.258744782168197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pressure_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>975.6323839315335</td>\n",
       "      <td>10.951366499099095</td>\n",
       "      <td>923.9123690476193</td>\n",
       "      <td>1012.156319047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearAcceleration_COV_z_x</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.01394454383825268</td>\n",
       "      <td>0.07354058303758416</td>\n",
       "      <td>-0.4230114247401415</td>\n",
       "      <td>0.12234128261081235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearAcceleration_COV_z_y</th>\n",
       "      <td>38</td>\n",
       "      <td>0.019247509213771653</td>\n",
       "      <td>0.09434167955021687</td>\n",
       "      <td>-0.10666261593735996</td>\n",
       "      <td>0.4071494486785601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearAcceleration_x_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.2846000911346868</td>\n",
       "      <td>0.463629657722139</td>\n",
       "      <td>-3.0134372887755103</td>\n",
       "      <td>-0.09735710558777227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearAcceleration_z_MEAN</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.47000978554165923</td>\n",
       "      <td>3.1532518479868483</td>\n",
       "      <td>-1.1425618128636363</td>\n",
       "      <td>18.45029201020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attack</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0                      1  \\\n",
       "summary                     count                   mean   \n",
       "_c0                            38                24498.5   \n",
       "GyroscopeStat_x_MEAN           38  -0.002919810384414524   \n",
       "GyroscopeStat_z_MEAN           38  -0.020669563899161792   \n",
       "GyroscopeStat_COV_z_x          38  -0.004745679981555173   \n",
       "GyroscopeStat_COV_z_y          38  0.0027225391041282225   \n",
       "MagneticField_x_MEAN           38     -828.5815522391483   \n",
       "MagneticField_z_MEAN           38    -20.569553774133187   \n",
       "MagneticField_COV_z_x          38     3.6145847662909802   \n",
       "MagneticField_COV_z_y          38      4.417610595486042   \n",
       "Pressure_MEAN                  38      975.6323839315335   \n",
       "LinearAcceleration_COV_z_x     38   -0.01394454383825268   \n",
       "LinearAcceleration_COV_z_y     38   0.019247509213771653   \n",
       "LinearAcceleration_x_MEAN      38    -0.2846000911346868   \n",
       "LinearAcceleration_z_MEAN      38   -0.47000978554165923   \n",
       "attack                         38                    1.0   \n",
       "\n",
       "                                               2                     3  \\\n",
       "summary                                   stddev                   min   \n",
       "_c0                           11.113055385446435                 24480   \n",
       "GyroscopeStat_x_MEAN        0.042653074223189466  -0.12934446667563454   \n",
       "GyroscopeStat_z_MEAN         0.06558910434958976   -0.4005862796940704   \n",
       "GyroscopeStat_COV_z_x        0.03900978043943563   -0.2293913810431748   \n",
       "GyroscopeStat_COV_z_y        0.04253071796921174  -0.09230434233181373   \n",
       "MagneticField_x_MEAN           5068.768853339744              -31252.0   \n",
       "MagneticField_z_MEAN          16.276224724990833    -61.70115789473685   \n",
       "MagneticField_COV_z_x          10.42308323200004    -45.43701104356637   \n",
       "MagneticField_COV_z_y          6.088725194969065    -7.984286527514232   \n",
       "Pressure_MEAN                 10.951366499099095     923.9123690476193   \n",
       "LinearAcceleration_COV_z_x   0.07354058303758416   -0.4230114247401415   \n",
       "LinearAcceleration_COV_z_y   0.09434167955021687  -0.10666261593735996   \n",
       "LinearAcceleration_x_MEAN      0.463629657722139   -3.0134372887755103   \n",
       "LinearAcceleration_z_MEAN     3.1532518479868483   -1.1425618128636363   \n",
       "attack                                       0.0                     1   \n",
       "\n",
       "                                               4  \n",
       "summary                                      max  \n",
       "_c0                                        24517  \n",
       "GyroscopeStat_x_MEAN           0.192444842939196  \n",
       "GyroscopeStat_z_MEAN         0.01580415402964103  \n",
       "GyroscopeStat_COV_z_x        0.05479838414699555  \n",
       "GyroscopeStat_COV_z_y        0.23981542270362485  \n",
       "MagneticField_x_MEAN          42.310158730158726  \n",
       "MagneticField_z_MEAN           7.576736842105262  \n",
       "MagneticField_COV_z_x         29.967837894736828  \n",
       "MagneticField_COV_z_y         22.258744782168197  \n",
       "Pressure_MEAN                  1012.156319047619  \n",
       "LinearAcceleration_COV_z_x   0.12234128261081235  \n",
       "LinearAcceleration_COV_z_y    0.4071494486785601  \n",
       "LinearAcceleration_x_MEAN   -0.09735710558777227  \n",
       "LinearAcceleration_z_MEAN      18.45029201020408  \n",
       "attack                                         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1586823, 13)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureColumns= ['GyroscopeStat_x_MEAN', 'Pressure_MEAN', 'LinearAcceleration_z_MEAN', 'GyroscopeStat_z_MEAN']\n",
    "dfWN=df.na.drop()\n",
    "dfWN.count(), len(dfWN.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"GyroscopeStat_x_MEAN\" does not exist.\\nAvailable fields: rowID, hpwren_timestamp, air_pressure, air_temp, avg_wind_direction, avg_wind_speed, max_wind_direction, max_wind_speed, min_wind_direction, min_wind_speed, rain_accumulation, rain_duration, relative_humidity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1121.transform.\n: java.lang.IllegalArgumentException: Field \"GyroscopeStat_x_MEAN\" does not exist.\nAvailable fields: rowID, hpwren_timestamp, air_pressure, air_temp, avg_wind_direction, avg_wind_speed, max_wind_direction, max_wind_speed, min_wind_direction, min_wind_speed, rain_accumulation, rain_duration, relative_humidity\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9eeb220757d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfeatureColumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0massembled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0massembled2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"GyroscopeStat_x_MEAN\" does not exist.\\nAvailable fields: rowID, hpwren_timestamp, air_pressure, air_temp, avg_wind_direction, avg_wind_speed, max_wind_direction, max_wind_speed, min_wind_direction, min_wind_speed, rain_accumulation, rain_duration, relative_humidity'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler= VectorAssembler (inputCols= featureColumns, outputCol= \"features\")\n",
    "assembled = assembler.transform(df)\n",
    "assembled2 = assembler.transform(df2)\n",
    "\n",
    "(trainingData, testData) = assembled.randomSplit([0.8,0.2], seed= 13234)\n",
    "(trainingData2, testData2) = assembled2.randomSplit([0.8,0.2], seed= 13234)\n",
    "\n",
    "trainingData = trainingData.union(trainingData2)\n",
    "testData = testData.union(testData2)\n",
    "trainingData2.count(), testData2.count(), \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt= DecisionTreeClassifier (labelCol=\"attack\", featuresCol=\"features\",\n",
    "                           maxDepth=5, minInstancesPerNode=20,\n",
    "                           impurity=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline= Pipeline(stages=[dt])\n",
    "model = pipeline.fit (trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction|attack|\n",
      "+----------+------+\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"attack\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\", \"attack\").write.save(path=\"file:///home/fran/Descargas/p2/predictions\",format=\"com.databricks.spark.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions=sqlCtx.read.load('file:///home/fran/Descargas/p2/predictions', format='com.databricks.spark.csv', header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction|attack|\n",
      "+----------+------+\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "|       0.0|     0|\n",
      "+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuraccy = 0.998367\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator( labelCol=\"attack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracy= evaluator.evaluate(predictions)\n",
    "print(\"Acuraccy = %g\" % (acuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "#predictions = (predictions.withColumn(\"attack\", predictions[\"attack\"].cast(\"double\")))\n",
    "metrics= MulticlassMetrics(predictions.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4892.,    8.],\n",
       "       [   0.,    0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray().transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
